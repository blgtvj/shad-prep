# Линейная алгебра

Книги и задачники:

- S. Axler. Linear Algebra Done Right, Second Edition, Springer, 1997 (или любое последующее издание)
- Кострикин Манин. Линейная алгебра. любое издание.

- Сборник задач по алгебре под редакцией А.Н. Кострикина. Новое издание. М.: МЦНМО, 2009.
- И.В. Проскуряков. Сборник задач по линейной алгебре (любое издание, например М.: БИНОМ, 2005)
- Г.Д. Ким, Л.В. Крицков. Алгебра и аналитическая геометрия. Теоремы и задачи. Том I. М.: "Планета знаний", 2007.

Еще:

- Прасолов, Задачи и теоремы линейной алгебры — <https://www.mccme.ru/prasolov/>, <http://staff.math.su.se/mleites/books/prasolov-1994-problems.pdf>  
- [Linear Algebra Problem Book](https://www.amazon.com/Algebra-Problem-Dolciani-Mathematical-Expositions/dp/0883853221) by Paul Halmos  

Видео:  

- Gilbert Strang [lectures on Linear Algebra](https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D) — здесь нет много чего, например, собственных значений, но начала разобраны

# LaTeX в чате

https://www.codecogs.com/latex/eqneditor.php — работает в слэке, но не в телеграме  

Можно еще попробовать:  
https://stackedit.io  
https://hackmd.io/  
http://www.texpaste.com/ — падает иногда  
https://www.hostmath.com  

Распознавание рукописной математики:  
https://webdemo.myscript.com/views/math/index.html  
https://mathpix.com  
http://detexify.kirelabs.org/classify.html — распознавание математических символов  






На ютубе таймстэмпы в комментариях кликабельные. Здесь копии на всякий случай.

# Таймстэмпы — Оглавление

- [ШАД prep — линал 01](#шад-prep--линал-01)
- [ШАД prep — линал 02](#шад-prep--линал-02)
- [ШАД prep — линал 03](#шад-prep--линал-03)
- [ШАД prep — линал 04](#шад-prep--линал-04)
- [ШАД prep — линал 05](#шад-prep--линал-05)
- [ШАД prep — линал 06](#шад-prep--линал-06)
- [ШАД prep — линал 07](#шад-prep--линал-07)
- [ШАД prep — линал 08](#шад-prep--линал-08)

# ШАД prep — линал 01  
  
https://www.youtube.com/watch?v=qpbDzZETNhA  

00:41 пара слов про язык конкретный и язык абстрактный в линейной алгебре  
03:46 вспоминаем метод Гаусса  
11:53 количество решений в зависимости от количества свободных переменных  
16:18 связь между Ax = b и Ax = 0  
  
20:05 задача ДЗ-01--4: задача про любовь   
31:15 вариант подзадачи, где иксы на обеих диагоналях  
  
37:17 задача ДЗ-01--5, но мы ее пропустили — решается выписыванием mn уравнений с k неизвестными
  
38:12 операции над матрицами  
40:48 проверка того, что (AB)^T = B^T A^T  
  
42:04 что такое tr(A)  
42:45 tr(AB) = tr(BA)  
  
45:02 матрицы как новый вид чисел, матричное уравнение Ax = b — все три участника уравнения являются матрицами, trade off между много про простые объекты и мало про сложные  
  
46:45 дефекты матриц, которыми надо пользоваться, когда вас просят придумать какие-нибудь примеры или сказать, бывает что-то или нет  
47:22 (1) AB != BA, когда придумываете примеры, используйте как можно больше нулей  
48:08 если у А в прозведении AB есть нулевая строка, то и в результате будет нулевая строка  
50:06 (2) AB = 0 — это чуть более сложный пример матричного уравнения Ax = 0  
51:32 диагональные матрицы перемножаются покомпонентно  
51:57 (3) нильпотент A^2 = 0  
52:23 это три примера, которые нужно иметь ввиду, когда просят придумать какой-нибудь контрпример  
  
53:45 если A — диагональная, то какие X с ней коммутируют, AX = XA? Только диагональные   
55:18 умножение на диагональную матрицу слева — построчное умножение на элементы ее диагонали, а справа — постолбцовое  
57:32 простой пример 2x2 для иллюстрации про коммутирующие с диагональной  
  
59:32 какие матрицы коммутируют с диагональной матрицей, когда у нее сгруппированы повторяющиеся элементы на диагонали? — блочныы матрицы, у которых вне диагонали блоки нулевые, а на диагонали блоки произвольные  
1:03:08 просьба объяснить, как про только диагональные коммутируют с диагональными доказать строго — ответ: напрямую через индексы  
1:04:29 что будет, если элементы повторяются на диагонали, но не сгруппированы  
  
1:06:05 произведение на матрицу J(0), то есть на такую, у которой все нули, а на диагонали над главной единицы  
1:07:43 произведение на нее слева выталкивает матрицу наверх, справа — направо  
1:09:17 задача: какие матрицы коммутируют с J(0)? То есть, A J(0) = J(0) A   
1:18:19 задача: какие матрицы коммутируют с J(λ)?  
1:20:48 если что-то коммутирует с матрицей A, то будет коммутировать и с A + \lambda E  
1:22:32 замечание: J(λ) — жорданова клетка, важная штука, и если понимать, как они устроены, и что с ними коммутирует, то сразу решается большой блок задач  
  
1:28:30 задача: чему равно J(λ)^n?  
  
1:38:23 если матрицы — это новый вид чисел, то не хватает деления, но нельзя придумать операцию деления для всех ненулевых матриц, например, когда для ненулевых A и B произведение AB = 0  
1:40:22 определение обратной матрицы  
1:42:48 примеры обратимых матриц: для единичной, для диагональной  
1:43:41 примеры необратимых матриц: нулевая, с нулевой строкой, Ax = 0 с ненулевым решением  
1:46:14 условия на столбцы можно переформулировать на существование решения Ax = 0, которое автоматически означает необратимость  
  
1:49:39 блочное произведение матриц  
1:52:13 произведение AB можно представить в виде A ( B_1 | B_2 | ... | B_n ) = ( A B_1 | A B_2 | ... | A B_n )  
1:53:24 произведение можно превратить в сумму, X Y^t = X_1 Y_1^t + ... + X_n Y_n^t, если мы обе матрицы запишем в виде столбцов (X_1 | ..., X_n), (Y_1, ..., Y_n)   
1:54:55 это можно использовать для, например, получения следа произведения: tr(A + B) = tr(A) + tr(B); или наоборот, превратить сумму в произведение для вычисления определителя  
1:56:44 если в блочной форме есть нулевые блоки, то это упрощает результат  
  
1:58:39 односторонняя обратимость  
2:01:39 широкая матрица может быть обратима только справа, высокая — только слева  
2:02:08 пример обратимой справа, но не слева: (1 0)  
2:02:46 более того, у прямоугольных обратимых матриц обратных много: (1 0) (1 x)^t — если есть одна, то их всегда много  
  
2:04:56 набор эквивалентных утверждений: про единственность ненулевого решения , обратимость, линейную независимость, определитель, разложение в элементарные пр-я  
  
2:14:37 простое док-во того, что обратимая с обеих сторон матрица является квадратной: через след  
2:15:53 если есть обратные слева и справа, то они равны: B_1 ( A B_2 ) = ( B_1 A ) B_2 — левая часть равна B_1, правая равна B_2  
2:16:50 еще способ док-ва: широкая матрица не может быть обратима слева, высокая — справа  
2:25:21 еще способ док-ва: пусть для широкой A есть обратная B, тогда матрица AB обратима, и ABx = 0 должно иметь ровно одно решение — нулевое, но у Ax = 0 всегда есть ненулевое решение  
  
2:28:17 задача: есть нильпотент A^n = 0, найти (E - A)^{-1}  
2:32:37 еще решение, через геометрическую прогрессию  
  
2:38:47 tr( C A C^{-1} ) = tr(A)  
  
2:39:10 задача ШАД-08-06-2014--1 — подсказка  

# ШАД prep — линал 02

2:05 многочлен от матриц  
3:55 три утверждения про многочлены от матриц  
6:07 (1) любая матрица является корнем какого-то многочлена — теорема Гамильтора-Кэли  
11:18 (2) сопряжение выносится, g(C^{-1} A C) = C^{-1} g(A) C для обратимых C  
15:01 (3) для мн-нов f(x), g(x) матрицы f(A), g(A) коммутируют  
16:34 примеры, как этими утвержднениями пользоваться: 1) если A^n = E, существует ли обратная к E-A? 2) f(A) = 0, f(0) != 0, тогда A обратима — найдем явный вид обратной  
  
24:26 спектр — определение через необратимость  
26:42 самый простой пример: спектр диагональной матрицы есть мн-во эл-тов на диагонали  
27:47 пример: spec J(λ) = {λ}, как показать это без использования определителя  
36:03 пример: спектр верхнетреугольной матрицы — тоже мн-во эл-тов на диагонали  
38:53 пример: матрица с пустым вещественным спектром  
  
42:26 для чего нужен спектр: если утилитарно, то половина задач — это задачи про собственные значения и спектр, если математически, то это важный инвариант, используемый на практике  
  
44:23 минимальный многочлен  
46:44 четыре свойства:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) мин. мн-н существует и единственный: ∃! f_min размерности deg f_min ≤ n  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) мин. мн-н делит зануляющие g(A) = 0 ⇒  f_min | g  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) спектр — это набор корней мин. мн-на: spec_{ℝ}(A) = {корни f_min в ℝ}  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) спектр — подмн-во корней зануляющего: g(A) = 0 ⇒ spec_{ℝ}(A) ⊆ {корни g в ℝ}  
50:06 мин. мн-н диагональной матрицы с различными эл-тами  
53:42 зачем нужен четвертый пункт: просто чтобы явно его произнести, он следует из предыдущих; обычно зануляющий довольно несложно найти A^n = E дает g(x) = x^n - 1  
  
56:09 как искать минимальный многочлен — это не сложно, на практике вряд ли понадобится  
  
1:09:56 диагонализуемость — сложный концепт для аудитории, много вопросов возникло  
1:20:19 пример: как решить A^2 = E, ее зануляющий g(x) = x^2 - 1 = (x-1)(x+1),  тогда можно диагонализовать и даже осортировать единицы, C^{-1} A C = ( E_k 0 \\ 0 -E_l ), k+l = n, т.е. таких разложений какое-то количество, а таких C вообще бесконечное количество, значит и решений таких матричных уравнений тоже бесконечное количество  
1:35:08 пример, когда диагонализуемость не работает из-за кратных решений зануляющего мн-на: J(0), его зануляющий g(x) = x^2; J(λ), его зануляющий g(x) = (x - λ)^2  
1:37:25 жорданова клетка J(λ) — один из главных контрпримеров, она показывает много аномалий — спектром матрица не определяется, еще важен инвариант минимальный мн-н   
  
1:39:56 задача дз-1-9:, найти мн-во матриц, коммутирующих со всеми, AX = XA, ответ — X = λE  

1:52:02 задача 18.17: д-ть, что E - AB обратима ⇔ E - BA обратима  
2:01:14 важно, что здесь A и B разного размера; иногда надо проверить обратимость (E - AB), но когда обратимость (E - BA) — это вообще матрица 1x1  
2:11:35 объяснение, для чего нужна абстрактная наука — она оборачивает сложные штуки в абстракции и дает к ним интерфейс  
2:14:00 объяснение, что мы сначала изучаем технические вещи про матрицы, а потом начнем изучать геометрические идеи, и нам будет намного проще, потому что у нас будет накоплено много примеров  
2:15:57 трюковое решение  
https://discourse.shad-prep-meetup-msk.ru/t/tryukovoe-reshenie-takoj-zadachi-e-ab-obratima-e-ba-obratima/48  
  
2:23:24 сравнение спектров AB и BA — если A высокая ▯, а B широкая ▭, то spec(AB) = {0} ∪ spec{BA}, они совпадают с точностью до включения нуля, а если матрицы квадратные, то спектры полностью совпадают — здесь включение нуля означает необратимость; когда изучим определители, будет апгрейд этого полезного утверждения  
  
2:37:13 минорная, но важная вещь: если произведение квадратных матриц A_1 ... A_n обратимо, то каждая из них обратима  

# ШАД prep — линал 03

00:00 концепция ориентированной площади и объема  
06:43 определитель матрицы — три определения  
19:50 примеры: определители матриц 1x1, 2x2, 3x3  
  
24:07 пример: как считать определитель, используя элементарные преобразования   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) можно прибавлять строку с умножением ее на коэффициент  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) можно выносить число из строки  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) перестановки строк меняют знак определителя  
  
28:58 эквивалентные определения невырожденности — сюда добавляется определитель  
  
36:02 пример: определитель диагональной матрицы, определитель верхнетреугольной матрицы  
38:14 пример: det(λ 1 1 \\ 1 λ 1 \\ 1 1 λ) — здесь, как во множестве других примеров, надо заметить какую-то закономерность  
44:21 пример: такой же, как выше, только чуть больше лямбд, та же закономерность  
  
47:32 св-ва определителя:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;det( A^t ) = det(A)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;det( A^{-1} ) = 1/det(A)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;det( AB ) = det(A) det(B)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;det( ( A B \\ 0 C) ) = det(A) det(C) — определитель блочной  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;det( λE ) = λ^n — это часто забывают и пишут просто λ по привычке  
  
53:16 пример: блочный определитель — det( A B \\ C D) = det(A) det( D - C A^{-1} B ), когда A обратима (здесь A — n×n, D — m×m). Получается умножением на матрицу элементарного преобразования (E 0 \\ -CA^{-1} E).  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Эта формула близка к той, которую очень хотелось бы: det( A B \\ C D) = det( AD - BC ), но во-первых, размеры A не позволяют внести ее во второй сомножитель, и во-вторых, A и C не обязательно коммутируют  
1:01:44 пример: блочный определитель, когда матрица разбита на равные по размеру блоки — det( A B \\ C D) = det(AD - CB), при AC=CA  
1:05:19 здесь работает принцип продолжения по непрерывности: рассмотрим не матрицу A, а матрицу A_λ = A - λE  
1:07:46 важное свойство спектра: он конечен  
  
1:22:00 в этом блочном определителе мы пользуемся тем, что A и C коммутируют. Любые две соседние сойдут на самом деле: A и C, A и B, B и D, C и D. Но не диагональные.  
  
1:23:51 пример, как можно попробовать свернуть сумму в произведение, когда квадратная матрица X = ( X_1 | ... | X_n ): det(λ_1 X_1 X_1^t + ... + λ_n X_n X_n^t) = det( X diag(λ_1, ..., λ_n) X^t ) = det(X)^2 λ_1, ..., λ_n  
  
1:30:59 задача ШАД-09-06-2018--5: доказать, что для ортогональных матриц A и B: det( A^t B - B^t A) = det( A+B ) det( A-B ) — матрица ортогональна, когда M M^t = M^t M = E  
  
1:39:47 разложение по строке или столбцу  
  
1:44:50 нахождение обратной через присоединенную матрицу, \hat{B} B = B \hat{B} = det(B) E  
1:50:54 det (\hat{B}) det(B) = det(B)^n, то есть, det (\hat{B}) = det(B)^{n-1} при det(B) ≠ 0  
1:52:00 формула верна и при det(B) = 0, потому что det (B) = 0  ⇒ det (\hat{B}) = 0  
  
1:58:25 характеристический многочлен χ_A(λ) = det( λE - A )  
1:59:19 (1) χ_A(λ) = λ^n - tr(A) λ^{n-1} + ... + (-1)^n det(A) — то, что скрывается за многоточием вряд ли понадобится  
2:03:08 (2) спектр — это корни характеристического многочлена  
2:04:35 (3) теорема Гамильтона-Кэли  
2:08:24 минимальный делит характерестический — это альтернативная формулировка т. Гамильтона-Кэли  
  
2:09:27 задача 31-05-2015--7: д-ть, что для квадратных матриц характеристические многочлены и спектр AB и BA равны, χ_{AB}(λ) = χ_{BA}(λ)  
2:15:59 если матрица A высокая ▯, а B широкая ▭, то spec(AB) = {0} ∪ spec{BA} и еще det(E - λAB) = λ^{n-m} det(E - λBA)   
2:23:41 пример, как пользоваться этим: det( E - αJvv^t ) = det( E - (αJv) (v^t) ) = χ_{AB}(1) = 1^{n-1} χ_{BA}(1)  

# ШАД prep — линал 04

0:00 конкретные векторные пространства   
5:45 абстрактное векторное пространство  
21:07 все хорошие векторные пространства устроены как R^n  
22:23 подпространства  
24:54 способы задавать: (1) через линейную оболочку < v_1, ..., v_k > = { λ_1 v_1 + ... + λ_k v_k }  
28:23 (2) через ОСЛУ { y | Ay = 0 }  
31:58 линейное отображение  
33:41 примеры: (1) все линейные отображения из R^n в R^m выглядят как x → Ax  
37:20 (2) все отображения матриц M_{nm}(R) → M_{uv}(R) выглядят как X → \Sigma A_i X B_i  
41:32 (3) отображение непрерывшых функций на отрезке [0, 1] в их интегралы  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) отображение непрерывшых функций на отрезке [0, 1] в функции на отрезке [0, 1]: например, f(x) → f(x) g(x), это произведение  
44:47 пояснение по пункту (2)  
  
с абстрактными штуками разобрались, к ним долго привыкать, и они выстрелят в следующий раз, теперь задачи  
  
46:08 линейная независимость; пример: sin(x) и cos(x)  
54:04 пример: sin(x), sin(2x), ..., sin(nx)  
1:10:59 1, sin(x), sin^2(x), sin^3(x), ..., sin^n(x)  
  
1:17:47 базис  
1:20:16 равносильные определения:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  (1) v_1, ..., v _n — линейно-независимы и их достаточное число, что любой вектор через них выражается  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  (2) v_1, ..., v _n — максимальное линейно-независимое множество  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  (3) v_1, ..., v _n — минимальное множество, такое что любой вектор через них выражается  
1:24:10 примеры базисов  
  
1:29:35 эквивалентные определения:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  столбцовый ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  строковый ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  факториальный ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  тензорный ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  минорный ранг  
1:43:13 замечания: случаи, когда rk A = 0 и rk A = 1  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  элементарные пр-я строк и столбцов не меняют ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  rk(A) = rk( C A ) = rk( A D ) при обратимых C, D  
1:47:13 столбцовый ранг равен строковому рангу  
1:50:58 факториальный ранг равен тензорному  
1:52:23 еще один пункт к списку определений невырожденности: rk A = n  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  и ранг — это мера невырожденности матрицы: rk A = n, n-1, ..., 1, 0  
  
1:55:22 пример: rk( A B \\ 2A -5B ) = rk A + rk B  
1:58:58 пример: про rk( A B \\ 0 0 ) мы не можем ничего утвевждать, там может быть много разных ситуаций  
2:01:11 пример: rk( A AB \\ B  B+B^2 ) = rk A + rk B  
  
2:02:45 верно ли, что rk( AB ) = rk( BA )? — контрпример: ( 1 0 \\ 0 0 ) ( 0 1 \\ 0 0 )  
2:04:27 разложение ( 0 1 \\ 0 0 ) в произведение столбца и строки: (1 0)^t (0 1)  
2:05:25 разложение матрицы в произведение двух матриц A = B C, где B — базис, набранный из столбцов A, а C — столбцы координат в этом базисе  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  найти эти коэффициенты не сложно, есть стандартные алгоритмы, но это не очень релевантно для экзамена  
  
2:08:57 пример: P — квадратная и P^2 = P, тогда rk P = tr P — то есть, у проекции след целый и равен размерности образа  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  это показывается через приведение зануляющего к диагональному виду  
2:12:29 пример: найти rk( P - λE )  
  
2:16:58 оценки на ранг  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  | rk A - rk B | ≤ rk(A+B) ≤ rk A + rk B  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  rk A + rk B - k ≤ rk(AB) ≤ min(rk A, rk B)  
  
2:20:30 задача: дана квадратная B, найти rk(\hat{B})   

# ШАД prep — линал 05

0:45 линейные отображения  
3:21 пара примеров  
8:30 векторные пространства  
  
18:32 пример: матрица оператора d/dx на пр-ве мн-в степени ≤ n  
24:40 пример: X -> AXB линейный оператор на пр-ве M_2 — это кронекерово произведение  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;это то же самое, что рассматривать оператор y -> (A⊗B)y в пр-ве R^4  
  
32:36 рассматриваем два базиса и как установить связь между ними  
  
42:29 векторизация матриц, M -> AMB можно переписать в умножение слева на матрицу, Y = CX   
  
44:55 пример: существует ли такое отображение, что вектор v_1 = (1 1)^t переходит в u_1 = (1 -1)^t, v_2 в u_2, v_3 в v_3, etc?   
  
59:15 смена координат в линейном пространстве: x' = C^{-1} x  
1:02:50 смена координат для линейного отображения V -> U: новая матрица будет A' = D^{-1} A C, где C — смена базиса в исходном пр-ве V, а D — смена базиса в целевом пр-ве U  
1:09:07 пояснение, что в C мы укладываем координаты нового басиса по столбцам  
1:11:52 как перейти из неприятного базиса сразу в неприятный за одно действие: B_1 C = B_2, тогда C = B_1^{-1} B_2  
1:16:49 еще раз то же самое в координатной форме  
  
1:22:42 линейный оператор в разных базисах, A' = C^{-1} A C  
1:26:12 пример: сопряжены ли данные матрицы? ( 1 0 \\ 0 0 ) и ( 0 0 \\ 0 1 )  
1:30:22 пример: сопряжены ли данные матрицы? ( 2 1 \\ 1 0 ) и ( 2 1 \\ 1 -1 )  
1:31:26 можно составить и решить ситему с четыремя неизвестными, а можно посмотреть на инварианты:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  tr(C^{-1} A C) = tr(A)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  det(C^{-1} A C) = det(A)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  χ_{C^{-1} A C} (t) = χ_A (t)   
  
1:40:09 ядро, образ — Im φ, Ker φ  
1:42:34 пример: φ: R^2 -> R — вектор (x y)^t переходит просто в x  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Im φ — вся ось x, то есть R, Ker φ — вся ось y   
1:46:53 пример: φ: R^3 -> R, схлопываем (x y z)^t в z  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  пример: φ: R^3 -> R^2, схлопываем (x y z)^t в (x y)^t  
  
1:50:14 образ и ядро в терминах матриц  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Im φ = { ( A_1 | ... | A_n ) (x_1, ..., x_n)^t | x \in R } = < A_1, ..., A_n > — линейная оболочка столбцов матриц  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Ker φ = { x | Ax=0 } — решение системы линейных уравнений  
  
1:55:10 еще раз: столбцы A — обрацы стандартного базиса (1, 0, 0, ...), (0, 1, 0, ...), ...  
  
1:56:09 связь между Ax = b и Ax = 0 в терминах отобажений: { x | Ax = b } = x_0 + { x | Ax = 0 }  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   Φ^{-1}(b) = x_0 + Ker Φ — прообраз b есть какое-то решение плюс ядро  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     
2:02:36 для неквадратных матриц мы всегда можем сменами базисов в исходном и в целевом пространствах перейти к диагональному виду, A' = D^{-1} A C, здесь C и D независимы, нет проблем, обсуждали ранее  
2:06:45 когда матрицы квадратные, то есть у нас линейный оператор, из пространства в себя же, то теперь это A' = C^{-1} A C, если мы шевелим C, то меняется и C^{-1} — не всегда можно привести матрицу к диагональному виду, но зато есть жорданова нормальная форма, о ней позже  
  
2:11:19 сюрьективность, инъективность, dim Ker Φ + dim Ker Φ = dim V  
2:15:18 пример: существует ли отображение, что <(1 1)^t> = Ker Φ, < (1 0 1)^t (1 1 1)^t > = Im Φ  
2:16:46 пример: дана матрица A = (1 2 3 4 \\ ... 16)_{4x4}, найдите rk(A^2019)  
2:18:54 лемма о стабилизации  
  
2:30:46 собственные значения и векторы  
2:37:56 как находить собственные значения  
2:42:12 кратность корней  
2:43:42 как находить собственные векторы  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  единственное место, где нужна ФСР  
2:45:49 пример: J_2(λ), ее х.м.  χ(t) = (t-λ)^2, собственное значение λ кратности 2  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  собственных векторов будет от одного до двух, если их нет, то где-то ошибка в вычислениях  
2:50:49 базис из собственных векторов  
  
2:52:12 кридерий диагонализируемости  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  матрица диагонализуется <=> (1) все с.зн. вещественные, (2) для каждого с.зн. размер базиса равен кратности  
2:59:11 жорданова нормальная форма  
3:02:49 над C это всегда работает, а чтобы это над R работало, надо чтобы все с.зн. были вещественными   
3:03:03 геометрический смысл столбцов C в A = C A_d C^{-1}, когда она диагонализуется:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  в столбцах C пачками стоят собственные векторы, соответствующие собственным значениям   
3:04:24 с жнф сложней, там не получится просто показать, что в C содержится, но для экзамена это не нужно  
3:06:01 еще замечание: если дана какая-то случайная матрица, то у ее хар. мн-на все корни будут разные кратности 1  

# ШАД prep — линал 06

00:55  вспоминаем, что такое векторное пространство, отображение, базис, собственные значения, собственные векторы, спектр, кратности корней  
16:12 задача: v ∈ R^n — найти собственные значения матрицы vv^t  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;для этого вспоминаем, что если матрица A высокая ▯, а B широкая ▭, то spec(AB) = {0} ∪ spec{BA}  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;и еще что det(E - λAB) = λ^{n-m} det(E - λBA)   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;док-во — третье видео, 2:15:59
  
26:27 проекторы  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P^2 = P — это алгебраическое определение  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;U+V = R^n, U ∩ V = ∅, P(V) = 0, P(u) = u — геометрическое  
33:51 эквивалентность этих определений  
39:32 прерываемся на вопрос: можно ли считать, что образ и ядро в некотором смысле перпендикулярны?  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ответ: нет. Пример: жорданова клетка J_2(0), ее ядро Ker и образ Im совпадают и равны <(1,0)^t> — горизонтальной прямой  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;эта матрица схлопывает всё в ось y и потом перекладывает на ось x  
49:19 продолжение про эквивалентность определений   
  
57:15 задача: P^2 = P, найти spec(P)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;из зануляющего следует, что это {0}, или {1}, или {0, 1}  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;вспоминаем, что P можно диагонализовать, размер единичной матрицы внутри после диагонализации r = rk R = tr P — след целый, это также кратность единицы как с.зн.  
  
1:09:07 пример: как решать ур-е X^2 = P — мы можем перейти к задаче Y^2 = (E 0 \\ 0 0)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;если интересно, как сконструировать матрицу C такую, что P = C (E 0 \\ 0 0) C^{-1}, то она состоит из базиса линейных оболочек <P> и <E-P>, это базисы образа и ядра  
  
1:16:09 задача ШАД-26-05-2018--7: здесь неудобно ее копипастить, так что ссылка: https://efiminem.github.io/supershad/26-05-2018/  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TODO: разметить идеи, использованные в разборе задачи  
  
1:52:22 задача ШАД-25-05-2014--1: Пусть A — квадратная матрица, у которой сумма матричных элементов в каждом столбце равна λ.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Докажите, что λ является собственным значением матрицы A.   
  
1:57:19 задача ШАД-02-06-2018--6: A^3 — это оператор проекции, то есть (A^3)^2 = A^3. Какие собственные значения может иметь A? Верно ли, что A будет иметь диагональную матрицу в каком-либо базисе R^n?   
  
1:58:57 пример недиагонализуемой матрицы — J(0) — потому что diag(a, b) имеет другой спектр, он при смене базиса не меняется  
2:02:55 пример матрицы, которая не диагонализуется в R, но приводится к диагональному в C: ( 0 -1 \\ 1 0 ) — ее хар. мн-н x^2 + 1   
2:07:05 вспоминаем критерий диагонализуемости  
2:11:14 если диагонализуется, то переход к собственному базису — матрица C выложена из собственных векторов  
2:18:07 вопрос из зала: что такое собственный базис   
  
2:27:14 продолжаем решать задачу про проектор — spec_A ⊆ {корни зануляющего x^6 - x^3} = {0, 1, e^{2pi/3 i}, e^{-2pi/3 i}}  
2:32:28 ставим эти числа на диагональ и получаем матрицу, которая диагонализуется в C  
2:35:08 мы можем превратить ее в вещественную — (a+bi 0 \\ 0 a-bi) = C (a -b \\ b a) C^{-1}  
2:41:14 мы привели пример матрицы, которая диагонализуется в C, но не в R  
2:48:28 простое решение задачи про A^3 проектор — пример, когда матрица не диагонализируется ни в C, ни в R — J(0), если нужен менее тривиальный, то diag(1, J(0))  
  
2:52:45 жорданова нормальная форма  
2:58:36 пример жнф  
3:03:36 мы знаем количество клеток, а чтобы найти размеры клеток, надо рассматривать корневые подпространства, но для экзамена это не понадобится  
3:06:26 но для полноты картины, вот определение: x — корневой вектор для λ, если ∃k: (A-λE)^k = 0   
  
3:08:46 еще раз лемма о стабилизации, но на этот раз наглядно видно из жнф, когда происходит стабилизация — когда занулится самая большая клетка J(0)  

# ШАД prep — линал 07

in progress

# ШАД prep — линал 08

in progress

# Symbols for copypasting

https://altcodeunicode.com/alt-codes-math-symbols/  
https://en.wikipedia.org/wiki/Mathematical_operators_and_symbols_in_Unicode  

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω
  
λ χ

∀ ∃ ∄   
⊆ ⊇  
∩ ∪  
⇒ ⇔ →  
≤ ≥ ≠  
∅ ∈ ∉  
∑





